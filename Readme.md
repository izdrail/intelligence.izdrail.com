# Intelligence

This project provides a Dockerized setup of [Ollama](https://ollama.ai) with support for running the **Gemma3** model.  
The included **Makefile** makes it easy to build, run, test, and manage the container.


## âš™ï¸ Requirements

- [Docker](https://docs.docker.com/get-docker/) installed and running
- (Optional) [jq](https://stedolan.github.io/jq/) for pretty-printing JSON in the test output

---

## ğŸ“¦ Makefile Targets

### ğŸ”¨ Build the image
```bash
make build
````

Builds the Docker image using the name defined in the Makefile (`izdrail/intelligence.izdrail.com`).

---

### ğŸš€ Run the container

```bash
make run
```

Starts the container in detached mode, exposing **port 11434** for the Ollama API.

---

### ğŸ›‘ Stop and remove the container

```bash
make stop
```

---

### ğŸ”„ Restart the container

```bash
make restart
```

Stops the container (if running) and starts it again.

---

### ğŸ§ª Test the model

```bash
make test
```

Sends a test request to the Ollama API using the `gemma3:1b` model.
Example response (non-streaming):

```json
{
  "model": "gemma3:1b",
  "created_at": "2025-09-18T12:34:56Z",
  "response": "The boy listens to music in the green house.",
  "done": true
}
```

---

### ğŸ§¹ Clean up

```bash
make clean
```

Stops the container and removes the Docker image.

---

## ğŸ“¡ API Usage

You can interact with the containerized Ollama server via HTTP:

```bash
curl -s http://localhost:11434/api/generate \
  -d '{
    "model": "gemma3:1b",
    "prompt": "Tell me a short joke about russians.",
    "stream": false
  }' | jq .
```



